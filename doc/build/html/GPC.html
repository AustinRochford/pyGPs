
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Basic Classification &mdash; pyGPs 1.0.0 documentation</title>
    
    <link rel="stylesheet" href="_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '',
        VERSION:     '1.0.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="pyGPs 1.0.0 documentation" href="index.html" />
    <link rel="up" title="Demos" href="Examples.html" />
    <link rel="next" title="Sparse Classification" href="GPC_FITC.html" />
    <link rel="prev" title="Sparse Regression" href="GPR_FITC.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="GPC_FITC.html" title="Sparse Classification"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="GPR_FITC.html" title="Sparse Regression"
             accesskey="P">previous</a> |</li>
        <li><a href="index.html">pyGPs 1.0.0 documentation</a> &raquo;</li>
          <li><a href="Examples.html" accesskey="U">Demos</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="basic-classification">
<h1>Basic Classification<a class="headerlink" href="#basic-classification" title="Permalink to this headline">¶</a></h1>
<p>The demo shown in this tutorial can be obtained by running <em>pyGPs/Demo/demo_GPC.py</em>.</p>
<div class="section" id="load-data">
<h2>Load data<a class="headerlink" href="#load-data" title="Permalink to this headline">¶</a></h2>
<p>First, we import the data:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># GPC target class are +1 and -1</span>
<span class="n">demoData</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">&#39;data_for_demo/classification_data.npz&#39;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">demoData</span><span class="p">[</span><span class="s">&#39;x&#39;</span><span class="p">]</span>            <span class="c"># training data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">demoData</span><span class="p">[</span><span class="s">&#39;y&#39;</span><span class="p">]</span>            <span class="c"># training target</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">demoData</span><span class="p">[</span><span class="s">&#39;xstar&#39;</span><span class="p">]</span>        <span class="c"># test data</span>
</pre></div>
</div>
<p>The <img class="math" src="_images/math/aaa4de95cc6371d125d5cf773cdd9f9913dd1d8e.png" alt="120"/> data points were generated from two Gaussians with different means and covariances. One Gaussian is isotropic and contains
<img class="math" src="_images/math/38bd36e353df831303c0de895a9ec614cf3e7202.png" alt="2/3"/> of the data (blue), the other is highly correlated and contains <img class="math" src="_images/math/217aedbdc339bacc8ba075a2ec16902b098194e3.png" alt="1/3"/> of the points (red).
Note, that the labels for the targets are specified to be <img class="math" src="_images/math/7cfbd35086ffefb83637d42166582b43c8a1ff4a.png" alt="\pm 1"/> (and not <img class="math" src="_images/math/e15d84dfcdccf6b0d8fb485020852b5b0f4ea097.png" alt="0/1"/>).</p>
<p>In the plot, we superimpose the data points with the posterior equi-probability contour lines for the probability of the second class
given complete information about the generating mechanism.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/d2_1.png"><img alt="_images/d2_1.png" src="_images/d2_1.png" style="width: 560.0px; height: 420.0px;" /></a>
</div>
</div>
<div class="section" id="first-example-state-default-values">
<h2>First example <img class="math" src="_images/math/e12b6767375342ed57d27678e3ea1cdb97f47e15.png" alt="\rightarrow"/> state default values<a class="headerlink" href="#first-example-state-default-values" title="Permalink to this headline">¶</a></h2>
<p>Again, lets see the simplest use of gp classification at first</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">model</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">GPC</span><span class="p">()</span>             <span class="c"># binary classification (default inference method: EP)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>              <span class="c"># fit default model (mean zero &amp; rbf kernel) with data</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>            <span class="c"># optimize hyperparamters (default optimizer: single run minimize)</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>             <span class="c"># predict test cases</span>
</pre></div>
</div>
<p>Note, that inference is done via expectation propagation (EP) approximation by deault. How to set inference to Laplace approximation, see <a class="reference internal" href="#more-on-gpc"><em>A bit more things you can do</em></a>.</p>
</div>
<div class="section" id="second-example-gp-classification">
<h2>Second example <img class="math" src="_images/math/e12b6767375342ed57d27678e3ea1cdb97f47e15.png" alt="\rightarrow"/> GP classification<a class="headerlink" href="#second-example-gp-classification" title="Permalink to this headline">¶</a></h2>
<p>So we first state the model to be <img class="math" src="_images/math/192d69ed9c7bd67b3f34486786531bae08ba2d88.png" alt="GP"/> classification now:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">model</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">GPC</span><span class="p">()</span>
</pre></div>
</div>
<p>The rest is similar to GPR:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">k</span> <span class="o">=</span> <span class="n">cov</span><span class="o">.</span><span class="n">RBFard</span><span class="p">(</span><span class="n">log_ell_list</span><span class="o">=</span><span class="p">[</span><span class="mf">0.05</span><span class="p">,</span><span class="mf">0.17</span><span class="p">],</span> <span class="n">log_sigma</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">setPrior</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">setData</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">plotData_2d</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">,</span><span class="n">t1</span><span class="p">,</span><span class="n">t2</span><span class="p">,</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">ys</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">z</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">)))</span>
</pre></div>
</div>
<p><strong>[Theory]</strong>
In this example, we used an RBF kernel (squared exponential covariance function) with automatic relevance determination (ARD). This covariance function has one
characteristic length-scale parameter for each dimension of the input space (here <img class="math" src="_images/math/41c544263a265ff15498ee45f7392c5f86c6d151.png" alt="2"/> in total), and a signal magnitude parameter, resulting in
a total of <img class="math" src="_images/math/7cde695f2e4542fd01f860a89189f47a27143b66.png" alt="3"/> hyperparameters. ARD with separate length-scales for each input dimension is a very powerful tool to learn which
inputs are important for the predictions: if length-scales are short, input dimensions are very important, and when they grow very large
(compared to the spread of the data), the corresponding input dimensions will be mostly ignored.</p>
<p>Note, <em>GPC.plot()</em> is a toy method for 2-d data:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">model</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">,</span><span class="n">t1</span><span class="p">,</span><span class="n">t2</span><span class="p">)</span>
</pre></div>
</div>
<p>The contour plot for the predictive distribution is shown below. Note, that the predictive
probability is fairly close to the probabilities of the generating process in regions of high data density. Note also, that as you move
away from the data, the probability approaches <img class="math" src="_images/math/217aedbdc339bacc8ba075a2ec16902b098194e3.png" alt="1/3"/>, the overall class probability.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/d2_2.png"><img alt="_images/d2_2.png" src="_images/d2_2.png" style="width: 560.0px; height: 420.0px;" /></a>
</div>
<p>Examining the two ARD characteristic length-scale parameters after learning, you will find that they are fairly similar, reflecting the fact
that for this data set, both input dimensions are important.</p>
</div>
<div class="section" id="a-bit-more-things-you-can-do">
<span id="more-on-gpc"></span><h2>A bit more things you can do<a class="headerlink" href="#a-bit-more-things-you-can-do" title="Permalink to this headline">¶</a></h2>
<p>GPC uses expectation propagation (EP)  inference and Error function likelihood by default, you can explictly change to other methods:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">model</span><span class="o">.</span><span class="n">useInference</span><span class="p">(</span><span class="s">&quot;Laplace&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">useLikelihood</span><span class="p">(</span><span class="s">&quot;Logistic&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Basic Classification</a><ul>
<li><a class="reference internal" href="#load-data">Load data</a></li>
<li><a class="reference internal" href="#first-example-state-default-values">First example <img class="math" src="_images/math/e12b6767375342ed57d27678e3ea1cdb97f47e15.png" alt="\rightarrow"/> state default values</a></li>
<li><a class="reference internal" href="#second-example-gp-classification">Second example <img class="math" src="_images/math/e12b6767375342ed57d27678e3ea1cdb97f47e15.png" alt="\rightarrow"/> GP classification</a></li>
<li><a class="reference internal" href="#a-bit-more-things-you-can-do">A bit more things you can do</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="GPR_FITC.html"
                        title="previous chapter">Sparse Regression</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="GPC_FITC.html"
                        title="next chapter">Sparse Classification</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="_sources/GPC.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="GPC_FITC.html" title="Sparse Classification"
             >next</a> |</li>
        <li class="right" >
          <a href="GPR_FITC.html" title="Sparse Regression"
             >previous</a> |</li>
        <li><a href="index.html">pyGPs 1.0.0 documentation</a> &raquo;</li>
          <li><a href="Examples.html" >Demos</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2013, Marion Neumann, Shan Huang, Daniel Marthaler, Kristian Kersting.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.3.
    </div>
  </body>
</html>